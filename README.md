# Learning to play Tetris with Deep Reinforcement Learning

Variation Names and what is exactly used in each case, is hopefully explained thoroughly in the Dissertation Document ðŸ˜„

The main environment use was tetrisActions.py

The main folder used was Variation B agents.

Variation A and C make use of the tetris.py file and tetrisb.py which correspond to the full and small board respectively.

## To replicate any of the results, make a new folder with the files:
 - tetrisActions.py
 - plotting.py
 - all of the .py files of the agent of your choice or all of the agents in the same folder.
 - the notebook file.

Training ,evaluation plots and video generation are all in the notebook file.

The notebook file creates the repository and selects any of the agents based on the parameters you want to use.

For example by running the main.py in the folder "demo run" can replicate the training of the best performing agent.
